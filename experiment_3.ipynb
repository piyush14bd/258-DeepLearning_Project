{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67fd5295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ec911f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPERIMENT 3: Physics Formula Approximation\n",
    "# SHM: x(t) = A * cos(ω t)\n",
    "# Compare: MLP vs KAN (with and without auxiliary θ = ω t)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca013980",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Genarate Synthetic Data\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def generate_shm_dataset(\n",
    "    n_samples=6000,\n",
    "    A_range=(0.5, 5.0),\n",
    "    w_range=(0.5, 10.0),\n",
    "    t_range=(0.0, 5.0),\n",
    "    noise_std=0.0,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate samples of x(t) = A * cos(ω t) with optional Gaussian noise.\n",
    "    Returns:\n",
    "        A, w, t, x  (each of shape [N, 1], on CPU)\n",
    "    \"\"\"\n",
    "    A = torch.empty(n_samples, 1).uniform_(*A_range)\n",
    "    w = torch.empty(n_samples, 1).uniform_(*w_range)\n",
    "    t = torch.empty(n_samples, 1).uniform_(*t_range)\n",
    "\n",
    "    x = A * torch.cos(w * t)\n",
    "\n",
    "    if noise_std > 0:\n",
    "        x = x + noise_std * torch.randn_like(x)\n",
    "\n",
    "    return A, w, t, x\n",
    "\n",
    "# Generate data\n",
    "A, w, t, x = generate_shm_dataset(\n",
    "    n_samples=6000,\n",
    "    A_range=(0.5, 5.0),\n",
    "    w_range=(0.5, 10.0),\n",
    "    t_range=(0.0, 5.0),\n",
    "    noise_std=0.0,  # set >0 to test robustness\n",
    ")\n",
    "\n",
    "# Train / test split\n",
    "idx = torch.randperm(len(x))\n",
    "n_train = 5000\n",
    "train_idx = idx[:n_train]\n",
    "test_idx  = idx[n_train:]\n",
    "\n",
    "A_tr, w_tr, t_tr, x_tr = A[train_idx], w[train_idx], t[train_idx], x[train_idx]\n",
    "A_te, w_te, t_te, x_te = A[test_idx],  w[test_idx],  t[test_idx],  x[test_idx]\n",
    "\n",
    "# Push labels to device once\n",
    "y_tr = x_tr.to(device)\n",
    "y_te = x_te.to(device)\n",
    "\n",
    "print(\"Train samples:\", len(y_tr), \"| Test samples:\", len(y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac0042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Helper Metrics\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return torch.sqrt(nn.MSELoss()(y_pred, y_true)).item()\n",
    "\n",
    "def r2_score_torch(y_true, y_pred):\n",
    "    # y_* should be CPU numpy\n",
    "    return r2_score(y_true.detach().cpu().numpy(),\n",
    "                    y_pred.detach().cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe010bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] epoch 50/200 | train_loss=4.443302\n",
      "[MLP] epoch 100/200 | train_loss=4.289727\n",
      "[MLP] epoch 150/200 | train_loss=4.138612\n",
      "[MLP] epoch 200/200 | train_loss=4.027579\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     y_hat_mlp \u001b[38;5;241m=\u001b[39m mlp(Xte_mlp)\n\u001b[1;32m     40\u001b[0m mlp_rmse \u001b[38;5;241m=\u001b[39m rmse(y_te, y_hat_mlp)\n\u001b[0;32m---> 41\u001b[0m mlp_r2   \u001b[38;5;241m=\u001b[39m \u001b[43mr2_score_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_te\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat_mlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== MLP baseline (A, ω, t) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlp_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m, in \u001b[0;36mr2_score_torch\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mr2_score_torch\u001b[39m(y_true, y_pred):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# y_* should be CPU numpy\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r2_score(\u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      9\u001b[0m                     y_pred\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3) MLP Baseline (this is your block)\n",
    "# ------------------------------\n",
    "class SHMMLP(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden=64):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "mlp = SHMMLP().to(device)\n",
    "opt_mlp = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "Xtr_mlp = torch.cat([A_tr, w_tr, t_tr], dim=1).to(device)\n",
    "Xte_mlp = torch.cat([A_te, w_te, t_te], dim=1).to(device)\n",
    "\n",
    "EPOCHS_MLP = 200\n",
    "for ep in range(1, EPOCHS_MLP + 1):\n",
    "    mlp.train()\n",
    "    opt_mlp.zero_grad()\n",
    "    pred = mlp(Xtr_mlp)\n",
    "    loss = loss_fn(pred, y_tr)\n",
    "    loss.backward()\n",
    "    opt_mlp.step()\n",
    "    if ep % 50 == 0:\n",
    "        print(f\"[MLP] epoch {ep}/{EPOCHS_MLP} | train_loss={loss.item():.6f}\")\n",
    "\n",
    "mlp.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat_mlp = mlp(Xte_mlp)\n",
    "\n",
    "mlp_rmse = rmse(y_te, y_hat_mlp)\n",
    "mlp_r2   = r2_score_torch(y_te, y_hat_mlp)\n",
    "\n",
    "print(\"\\n=== MLP baseline (A, ω, t) ===\")\n",
    "print(f\"RMSE: {mlp_rmse:.4f}\")\n",
    "print(f\"R²  : {mlp_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927c9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ec110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 KAN Experiments\n",
    "\n",
    "\n",
    "from kan import KAN\n",
    "\n",
    "# θ = ω t\n",
    "theta_tr = (w_tr * t_tr).to(device)\n",
    "theta_te = (w_te * t_te).to(device)\n",
    "\n",
    "Xtr_aux = torch.cat([A_tr.to(device), theta_tr], dim=1)  # shape [N,2]\n",
    "Xte_aux = torch.cat([A_te.to(device), theta_te], dim=1)\n",
    "\n",
    "kan_aux = KAN(\n",
    "    width=[2, 32, 1],  # input dim 2: (A, θ)\n",
    "    grid=5,\n",
    "    k=3,\n",
    "    seed=0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dataset_aux = {\n",
    "    \"train_input\": Xtr_aux,\n",
    "    \"train_label\": y_tr,\n",
    "    \"test_input\":  Xte_aux,\n",
    "    \"test_label\":  y_te,\n",
    "}\n",
    "\n",
    "print(\"\\nTraining KAN with auxiliary θ = ω t ...\")\n",
    "kan_aux.fit(dataset_aux, opt=\"LBFGS\", steps=80, lamb=0.0, lamb_entropy=0.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat_aux = kan_aux(Xte_aux)\n",
    "\n",
    "kan_aux_rmse = rmse(y_te, y_hat_aux)\n",
    "kan_aux_r2   = r2_score_torch(y_te, y_hat_aux)\n",
    "\n",
    "print(\"\\n=== KAN (with θ = ω t) ===\")\n",
    "print(f\"RMSE: {kan_aux_rmse:.4f}\")\n",
    "print(f\"R²  : {kan_aux_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9893cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6976a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b422ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8d892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
